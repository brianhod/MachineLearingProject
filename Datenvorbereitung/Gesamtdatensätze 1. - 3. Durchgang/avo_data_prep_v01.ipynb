{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bwxo5lBzJ3rw"
   },
   "source": [
    "# Daten laden\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SQ6vEkz7J58A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "I1T2pK37Mmdz",
    "outputId": "4c139a37-6563-4d50-9995-6f7de0804dc0"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/avocado-updated-2020.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/avocado-updated-2020.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Patrick 07.10.2024: Daten im colab hochgeladen\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/avocado-updated-2020.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/avocado-updated-2020.csv\") # Patrick 07.10.2024: Daten im colab hochgeladen\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtCo0Sq0bpaM",
    "outputId": "694b394f-1c8a-40c2-c660-d7182f85cc63"
   },
   "outputs": [],
   "source": [
    "# Prüfung, ob der Datenrahmen Nullwerte enthält\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "KaD-KFOb89V4",
    "outputId": "80496e9c-962e-45e7-9045-4972b5752855"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_QR6Mi3TFGN"
   },
   "source": [
    "# Daten verstehen\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QeZ8SIwdaPw",
    "outputId": "0050c994-8f08-4d66-9a10-be03de0e0ea9"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4VZb64D8E12",
    "outputId": "2f734204-d437-43d7-f119-f49c72662264"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "jj0AqTSRTJW5",
    "outputId": "27d9dfd6-4b28-42a1-8eae-f763fc559c1c"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS0ezOTteJkJ",
    "outputId": "f1158cf4-ef73-4523-bb40-1657dd86474a"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "IrpQiXzVeXSf",
    "outputId": "b04a5b69-d72f-4d24-ddd8-28737f5e1307"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyO_mMR6sHkK"
   },
   "source": [
    "Brian\n",
    "\n",
    "|   Spalte      | Typ     | Beschreibung                                    |\n",
    "|---------------|---------|-------------------------------------------------|\n",
    "| date          | object  |                                                 |\n",
    "| average_price | float64 |                                                 |\n",
    "| total_volume  | float64 |                                                 |\n",
    "| 4046          | float64 | kleine/mittlere Bio-Hass-Avocados               |\n",
    "| 4225          | float64 | große Hass-Avocados aus nicht                   |   \n",
    "|               |         | biologischem Anbau                              |\n",
    "| 4770          | float64 | nicht-biologische extra große Hass-Avocados     |\n",
    "| total_bags    | float64 |                                                 |\n",
    "| small_bags    | float64 |                                                 |\n",
    "| large_bags    | float64 |                                                 |\n",
    "| xlarge_bags   | float64 |                                                 |\n",
    "| type          | object  |                                                 |\n",
    "| year          | int64   |                                                 |\n",
    "| geography     | object  |                                                 |\n",
    "\n",
    "\n",
    "1.\tÜberblick über die Daten\n",
    "  * 1.1.  4046: organic small/medium Hass Avocados (~3-5 oz)\n",
    "  * 1.2.  4225: non-organic large Hass Avocados (~8-10 oz)\n",
    "  * 1.2.  4770: non-organic extra large Hass Avocados (~10-15 oz)\n",
    "\n",
    "* https://www.kaggle.com/code/shaniquehines/avocados-case-study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FSvkFd34IzU"
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrNO80wF31-G"
   },
   "source": [
    "## DateEncoder date_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jky368xsYPfB"
   },
   "outputs": [],
   "source": [
    "# Spalte 'date' in Datumsformat umwandeln (falls sie als String vorliegt)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sortieren nach Datum (optional, um sicherzustellen, dass die Daten chronologisch sind)\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Berechnen des frühesten Datums im Datensatz\n",
    "min_date = df['date'].min()\n",
    "\n",
    "# Berechnen der Anzahl der Tage seit dem frühesten Datum\n",
    "df['date_number'] = (df['date'] - min_date).dt.days\n",
    "\n",
    "# Die Spalte 'date_number' an den Anfang verschieben\n",
    "cols = ['date_number'] + [col for col in df.columns if col != 'date_number']  # Neue Reihenfolge der Spalten\n",
    "df = df[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UNLhbKWX_o2"
   },
   "source": [
    "## LabelEncoder type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHqqbpjrarby"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "df['type'] = lb.fit_transform(df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WoI0knoZUpi",
    "outputId": "d02a6ec6-4860-4f2f-f4bb-bbcd5b135f2f"
   },
   "outputs": [],
   "source": [
    "# Patrick LabelEncoder für geography\n",
    "\n",
    "\n",
    "# Importieren der notwendigen Bibliotheken\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialisierung des LabelEncoders für die 'geography'-Spalte\n",
    "geo_encoder = LabelEncoder()\n",
    "# Anwenden des Encoders auf die 'geography'-Spalte\n",
    "df['geography_encoded'] = geo_encoder.fit_transform(df['geography'])\n",
    "\n",
    "# Anzeigen der ersten Zeilen des Datensatzes, um die Ergebnisse zu überprüfen\n",
    "print(df[['geography', 'geography_encoded']].head())\n",
    "\n",
    "# Optional: Speichern des modifizierten Datensatzes in eine neue CSV-Datei\n",
    "# avocado_data.to_csv(\"data/avocado_encoded.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLlPo4ob3vKO"
   },
   "source": [
    "## GeoDistanceEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eu4sl5rIZd7J",
    "outputId": "0368533e-e756-463d-9126-e9e29d1f78fc"
   },
   "outputs": [],
   "source": [
    "# Patrick GeoDistanceEncoder für geography\n",
    "\n",
    "# Importieren der notwendigen Bibliotheken\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Definieren der Städte mit ihren Koordinaten\n",
    "static_coordinates = [\n",
    "    ('Albany', 42.6526, -73.7562),\n",
    "    ('Atlanta', 33.749, -84.388),\n",
    "    ('Baltimore/Washington', 39.2904, -76.6122),\n",
    "    ('Boise', 43.615, -116.2023),\n",
    "    ('Boston', 42.3601, -71.0589),\n",
    "    ('Buffalo/Rochester', 43.1566, -77.6088),\n",
    "    ('California', 36.7783, -119.4179),\n",
    "    ('Charlotte', 35.2271, -80.8431),\n",
    "    ('Chicago', 41.8781, -87.6298),\n",
    "    ('Cincinnati/Dayton', 39.1031, -84.512),\n",
    "    ('Columbus', 39.9612, -82.9988),\n",
    "    ('Dallas/Ft. Worth', 32.7767, -96.797),\n",
    "    ('Denver', 39.7392, -104.9903),\n",
    "    ('Detroit', 42.3314, -83.0458),\n",
    "    ('Grand Rapids', 42.9634, -85.6681),\n",
    "    ('Great Lakes', 45.0, -84.0),\n",
    "    ('Harrisburg/Scranton', 40.2732, -76.8867),\n",
    "    ('Hartford/Springfield', 41.7627, -72.6743),\n",
    "    ('Houston', 29.7604, -95.3698),\n",
    "    ('Indianapolis', 39.7684, -86.1581),\n",
    "    ('Jacksonville', 30.3322, -81.6557),\n",
    "    ('Las Vegas', 36.1699, -115.1398),\n",
    "    ('Los Angeles', 34.0522, -118.2437),\n",
    "    ('Louisville', 38.2527, -85.7585),\n",
    "    ('Miami/Ft. Lauderdale', 25.7617, -80.1918),\n",
    "    ('Midsouth', 35.1495, -90.0489),\n",
    "    ('Nashville', 36.1627, -86.7816),\n",
    "    ('New Orleans/Mobile', 29.9511, -90.0715),\n",
    "    ('New York', 40.7128, -74.006),\n",
    "    ('Northeast', 41.5, -75.0),\n",
    "    ('Northern New England', 44.0, -71.5),\n",
    "    ('Orlando', 28.5383, -81.3792),\n",
    "    ('Philadelphia', 39.9526, -75.1652),\n",
    "    ('Phoenix/Tucson', 33.4484, -112.074),\n",
    "    ('Pittsburgh', 40.4406, -79.9959),\n",
    "    ('Plains', 39.0119, -98.4842),\n",
    "    ('Portland', 45.5152, -122.6784),\n",
    "    ('Raleigh/Greensboro', 35.7796, -78.6382),\n",
    "    ('Richmond/Norfolk', 37.5407, -77.436),\n",
    "    ('Roanoke', 37.2709, -79.9414),\n",
    "    ('Sacramento', 38.5816, -121.4944),\n",
    "    ('San Diego', 32.7157, -117.1611),\n",
    "    ('San Francisco', 37.7749, -122.4194),\n",
    "    ('Seattle', 47.6062, -122.3321),\n",
    "    ('South Carolina', 33.8361, -81.1637),\n",
    "    ('South Central', 34.7465, -92.2896),\n",
    "    ('Southeast', 32.3182, -86.9023),\n",
    "    ('Spokane', 47.6588, -117.426),\n",
    "    ('St. Louis', 38.627, -90.1994),\n",
    "    ('Syracuse', 43.0481, -76.1474),\n",
    "    ('Tampa', 27.9506, -82.4572),\n",
    "    ('Total U.S.', 39.8283, -98.5795),\n",
    "    ('West', 39.1178, -120.013),\n",
    "    ('West Tex/New Mexico', 31.9686, -99.9018)\n",
    "]\n",
    "\n",
    "\n",
    "class GeoDistanceEncoder:\n",
    "    def __init__(self, city_coordinates, reference_points):\n",
    "        \"\"\"\n",
    "        Initialisiert den GeoDistanceEncoder mit den Städten und den Referenzpunkten.\n",
    "\n",
    "        :param city_coordinates: Dictionary mit Städtenamen und deren (Latitude, Longitude)-Koordinaten\n",
    "        :param reference_points: Dictionary mit Referenzpunkten und deren (Latitude, Longitude)-Koordinaten\n",
    "        \"\"\"\n",
    "        self.city_coordinates = city_coordinates\n",
    "        self.reference_points = reference_points\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # fit ist nicht notwendig, wird aber für Kompatibilität bereitgestellt.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Berechnet die geografischen Distanzen zu den Referenzpunkten. Falls die Spalten 'latitude' und 'longitude'\n",
    "        nicht im Datensatz vorhanden sind, werden sie anhand der Stadtzuordnung berechnet.\n",
    "\n",
    "        :param X: DataFrame mit der Spalte 'geography' (Städte) und optional 'latitude' und 'longitude'\n",
    "        :return: DataFrame mit den berechneten Distanzen zu den Referenzpunkten\n",
    "        \"\"\"\n",
    "        # Überprüfen, ob die Spalten 'latitude' und 'longitude' bereits vorhanden sind\n",
    "        if 'latitude' not in X.columns or 'longitude' not in X.columns:\n",
    "            # Falls die Spalten nicht existieren, die Koordinaten der Städte hinzufügen\n",
    "            X['latitude'] = X['geography'].map(lambda x: self.city_coordinates.get(x, (None, None))[0])\n",
    "            X['longitude'] = X['geography'].map(lambda x: self.city_coordinates.get(x, (None, None))[1])\n",
    "\n",
    "        # Entfernen von Zeilen ohne gültige Koordinaten\n",
    "        X = X.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "        # Berechnen der Entfernungen zu den Referenzpunkten\n",
    "        for name, coords in self.reference_points.items():\n",
    "            X[f'dist_to_{name}'] = X.apply(lambda row: geodesic((row['latitude'], row['longitude']), coords).kilometers, axis=1)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Führt fit und transform direkt hintereinander aus.\n",
    "\n",
    "        :param X: DataFrame mit der Spalte 'geography' (Städte) und optional 'latitude' und 'longitude'\n",
    "        :return: DataFrame mit den berechneten Distanzen zu den Referenzpunkten\n",
    "        \"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "# Umwandeln der Liste in ein Dictionary für den Encoder\n",
    "city_coordinates = {city: (lat, lon) for city, lat, lon in static_coordinates}\n",
    "\n",
    "# Referenzpunkte (Kalifornien und Mexiko)\n",
    "reference_points = {\n",
    "    'california': (36.7783, -119.4179),\n",
    "    'mexico': (23.6345, -102.5528)\n",
    "}\n",
    "\n",
    "# Anwendung des GeoDistanceEncoders\n",
    "encoder = GeoDistanceEncoder(city_coordinates, reference_points)\n",
    "df = encoder.fit_transform(df)\n",
    "\n",
    "# Überprüfen der ersten Zeilen des Datensatzes\n",
    "print(df[['geography', 'latitude', 'longitude', 'dist_to_california', 'dist_to_mexico']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHIWv23SDnRq"
   },
   "source": [
    "## East/West Aufteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "HwFhqTWXWWqJ",
    "outputId": "f6c20a2c-808d-4a6a-e811-7153be4efebf"
   },
   "outputs": [],
   "source": [
    "# Aufteilung in Westküste und Ostküste\n",
    "def classify_region(dataset):\n",
    "    west_coast_regions = ['California', 'Portland', 'Seattle', 'Los Angeles', 'San Francisco']\n",
    "    east_coast_regions = ['New York', 'Boston', 'Miami', 'Philadelphia', 'Baltimore/Washington',\n",
    "                          'Atlanta', 'Tampa', 'Orlando', 'Hartford/Springfield', 'Raleigh/Greensboro', 'Richmond/Norfolk']\n",
    "    #closer_east_coast = ['Albany','Buffalo/Rochester','Charlotte','Chicago','Cincinnati/Dayton','Columbus',\n",
    "    #                     'Dallas/Ft. Worth','Detroit','Grand Rapids','Great Lakes','Harrisburg/Scranton','Houston',\n",
    "    #                     'Indianapolis','Jacksonville','Louisville','Midsouth','Nashville','New Orleans/Mobile',\n",
    "    #                  'Northeast','Northern New England','Pittsburgh','Plains','Roanoke','South Carolina',\n",
    "    #                     'South Central','Southeast','St. Louis','Syracuse','Total U.S']\n",
    "    #closer_west_coast = ['Boise','Las Vegas','Denver','Phoenix/Tucson','Sacramento','San Diego','Spokane','West Tex/New Mexico','West']\n",
    "\n",
    "    #merged_east_coast = list(set(east_coast_regions + closer_east_coast))\n",
    "    #merged_west_coast = list(set(west_coast_regions + closer_west_coast))\n",
    "\n",
    "    dataset['region'] = dataset['geography'].apply(\n",
    "    lambda x: 0 if any(west in x for west in west_coast_regions) else\n",
    "              (1 if any(east in x for east in east_coast_regions) else None)\n",
    "  )\n",
    "    return dataset\n",
    "\n",
    "df = classify_region(df)\n",
    "df.head(20)\n",
    "df = df.dropna(subset=['region'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dWKTsi3Ds1s"
   },
   "source": [
    "## OneHot Encoder Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imy3ZytT2ZUw"
   },
   "outputs": [],
   "source": [
    "def onehot_encode(df, column):\n",
    "  # Make a copy of the origianl DataFrame\n",
    "  df = df.copy()\n",
    "\n",
    "  # Create dummy variables (one-hot encoding)\n",
    "  dummies = pd.get_dummies(df[column])\n",
    "\n",
    "  # Convert dummy columns to integers (0 and 1)\n",
    "  dummies = dummies.astype(int)\n",
    "\n",
    "  # Concatenate the dummy variables to the original DataFrame\n",
    "  df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "  # Drop the original column from the DataFrame\n",
    "  df.drop(column, axis =1, inplace = True)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zp7BCOJdOAJr"
   },
   "outputs": [],
   "source": [
    "df = onehot_encode(df, 'geography')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0caUkedERIy"
   },
   "outputs": [],
   "source": [
    "# Überprüfung NaN-Werte\n",
    "\n",
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaq38bgd77SX",
    "outputId": "37c43bc1-7dac-44a8-ea6f-b88a4f0d8d3d"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdA3_g_XD6A6"
   },
   "source": [
    "# Spalten löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Jium_IwzePW",
    "outputId": "292504ad-0b14-4a2d-b4f5-937b3547b7c1"
   },
   "outputs": [],
   "source": [
    "# Entfernen der angegebenen Spalten\n",
    "df = df.drop(columns=['date','geography_encoded', 'latitude', 'longitude'])\n",
    "\n",
    "# Ausgabe der ersten Zeilen des Datensatzes zur Überprüfung\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YL6jK1Dn-mYg",
    "outputId": "0804cbac-fac4-4dee-ac05-45d2e6f10486"
   },
   "outputs": [],
   "source": [
    "# Entfernen der angegebenen Spalten\n",
    "df = df.drop(columns=['4046', '4225', '4770','total_bags', 'small_bags', 'large_bags', 'xlarge_bags'])\n",
    "\n",
    "# Ausgabe der ersten Zeilen des Datensatzes zur Überprüfung\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HUmlmB_u0Tf"
   },
   "source": [
    "# Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRRG4_jhuuKj",
    "outputId": "5a7ed0e5-7da3-481a-b497-3f061f0cda32"
   },
   "outputs": [],
   "source": [
    "# Annahme: Die Zielvariable, die wir vorhersagen wollen, ist 'average_price'\n",
    "# Und die Feature-Spalten sind alle anderen Spalten außer 'average_price'\n",
    "\n",
    "# Splitten in Train- und Test-Datensätze basierend auf dem Jahr\n",
    "train = df[df['year'] < 2020]  # Training Data (before 2020)\n",
    "test = df[df['year'] >= 2020]  # Testing Data (2020 and later)\n",
    "\n",
    "# Features (X) und Zielvariable (y) definieren\n",
    "X_train = train.drop(columns=['average_price'])\n",
    "y_train = train['average_price']\n",
    "\n",
    "X_test = test.drop(columns=['average_price'])\n",
    "y_test = test['average_price']\n",
    "\n",
    "# Ausgabe der Formen der Train- und Test-Datensätze\n",
    "print(\"Train-Daten: X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"Test-Daten: X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ai7UhOVEKyy"
   },
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVV7w8-P_NVO"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'average_price' is your target variable, and the rest are features\n",
    "columns_to_scale = ['date_number', 'total_volume', 'dist_to_california', 'dist_to_mexico']  # Exclude 'average_price' from scaling\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data only and transform it\n",
    "X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "\n",
    "# Use the already fitted scaler to transform the test data\n",
    "X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Qn3moAGClc7"
   },
   "outputs": [],
   "source": [
    "# Entfernen der angegebenen Spalten\n",
    "X_train = X_train.drop(columns=['year'])\n",
    "X_test = X_test.drop(columns=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "hManEsVvCYTf",
    "outputId": "674bdda4-5414-4eff-cd58-fa68225436d7"
   },
   "outputs": [],
   "source": [
    "#Output the shapes and first few rows of scaled data for verification\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "T1A0-tZvC_5g",
    "outputId": "91cd0c26-ab33-4d73-cfee-03409fe354a2"
   },
   "outputs": [],
   "source": [
    "#Output the shapes and first few rows of scaled data for verification\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP82-sVjF44F"
   },
   "source": [
    "# Datenspeicherung in Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahmSmY_HQEsG",
    "outputId": "6edcba1f-4b81-48c0-b750-950f5b01272d"
   },
   "outputs": [],
   "source": [
    "# DataFrame als Pickle-Datei speichern\n",
    "import pickle\n",
    "\n",
    "# Annahme: X_train, y_train, X_test, y_test sind bereits definiert\n",
    "\n",
    "# Speichern in eine Pickle-Datei\n",
    "with open('data/train_test_data.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, y_train, X_test, y_test), f)\n",
    "\n",
    "print(\"Die Daten wurden erfolgreich in die Pickle-Datei gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foYeF7NpSlLT"
   },
   "outputs": [],
   "source": [
    "# # Laden der gespeicherten Pickle-Datei\n",
    "# with open('data/train_test_data.pkl', 'rb') as f:\n",
    "#     X_train, y_train, X_test, y_test = pickle.load(f)\n",
    "\n",
    "# # Überprüfen der geladenen Daten\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
